# Import the library applied 
import numpy as np 
import pandas as pd  
import glob
import keras
from keras.layers import *
from keras.regularizers import *
from keras.optimizers import *
from keras_self_attention import SeqSelfAttention
import matplotlib.pyplot as plt   
from pandas import DataFrame
import csv
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split


nb_epoch = 1
# Run 150 epochs for each of the model to achieve our kaggle score
#######################################################################################
# Load data files
train_dir = './train/train/'
data = pd.read_csv("./train_kaggle.csv")

Y = np.asarray(data['Label'])
X = np.zeros((Y.shape[0], 1000, 102))
X_train_mlp = np.zeros((Y.shape[0], 102))

for i in data['Id']:
    x = np.load(train_dir+str(i)+'.npy')
    X[i][:x.shape[0]] = x
    X_train_mlp[i] = np.mean(x, axis=0)


X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=1234)
pca = PCA(n_components=32)
pca.fit(X_train_mlp)
X_train_mlp, X_val_mlp, _, _ = train_test_split(X_train_mlp, Y, test_size=0.1, random_state=1234)

del X, Y

X_train_cnn = X_train[:, :, :, np.newaxis]
X_train_rnn = X_train
X_train_attention = X_train[:, :, :, np.newaxis]
X_train_pca = pca.transform(X_train_mlp)

X_val_cnn = X_val[:, :, :, np.newaxis]
X_val_rnn = X_val
X_val_attention = X_val[:, :, :, np.newaxis]
X_val_pca = pca.transform(X_val_mlp)


#######################################################################################
# Pre-processing of test data

test_dir = './test/test/'
max_id =  len(glob.glob(test_dir+'*.npy'))
X_test = np.zeros((max_id, 1000, 102))
X_test_mlp = []
for i in range(max_id):
    x = np.load(test_dir+str(i)+'.npy')
    X_test_mlp.append(np.mean(x, axis=0))
    X_test[i][:x.shape[0]] = x

X_test_mlp = np.asarray(X_test_mlp)
X_test_cnn = X_test[:, :, :, np.newaxis]
X_test_rnn = X_test
X_test_attention = X_test[:, :, :, np.newaxis]

#######################################################################################
# MLP Model 1 architecture
inputs = keras.Input(shape=(102,))
x = keras.layers.Dense(256, activation='relu')(inputs)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(128, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(128, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(64, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(32, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
training= model.fit(X_train_mlp, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_mlp, Y_val))
# model1.save('mlp_model1.h5')
y_test = np.array(model.predict(X_test_mlp))

# #######################################################################################
# MLP Model 2 Architecture
inputs = keras.Input(shape=(32,))
x = keras.layers.Dense(256, activation='relu')(inputs)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(128, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(128, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(64, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(32, activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
training= model.fit(X_train_pca, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_pca, Y_val))
# model2.save('mlp_model2.h5')
X_test_mlp = pca.transform(X_test_mlp)
y_test = np.add(y_test, model.predict(X_test_mlp))

#######################################################################################
# CNN Model Architecture

inputs = keras.Input(shape=(1000, 102, 1))

x = keras.layers.Conv2D(32, kernel_size = (7, 5), strides=(3, 1), activation='tanh', batch_input_shape=(None, 1000, 102, 1), padding='valid')(inputs)
x = keras.layers.AveragePooling2D(pool_size=(2,2))(x)
x = keras.layers.BatchNormalization()(x)

x = keras.layers.Conv2D(32, kernel_size = (5, 3), strides=(3, 1), activation='tanh', padding='valid')(x)
x = keras.layers.AveragePooling2D(pool_size=(2,2))(x)
x = keras.layers.BatchNormalization()(x)

x = keras.layers.Conv2D(16, kernel_size = (3, 3), activation='tanh', padding='same')(x)
x = keras.layers.AveragePooling2D(pool_size=(2,2))(x)
x = keras.layers.BatchNormalization()(x)

x = keras.layers.Flatten()(x)

x = keras.layers.Dense(128, activation='relu')(x)
x = keras.layers.Dropout(rate=0.1)(x)
x = keras.layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_cnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_cnn, Y_val))
# model.save('cnn_model4.h5')
y_test = np.add(y_test, model.predict(X_test_cnn))

#######################################################################################
# LSTM Model 1 Architecture
inputs = keras.Input(shape=(1000, 102))
x = LSTM(units=128, return_sequences=True)(inputs)
x = Dropout(0.3)(x)
x = LSTM(units=64)(x)
x = Dropout(0.3)(x)
x = Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
training= model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_rnn, Y_val))
# model.save('lstm_model1.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))


#######################################################################################
# LSTM Model 2 Architecture
inputs = keras.Input(shape=(1000, 102))
x = keras.layers.CuDNNLSTM(64, batch_input_shape=(None, 1000, 102))(inputs)
x = keras.layers.RepeatVector(64)(x)
x = keras.layers.CuDNNLSTM(32, return_sequences=True)(x)
x = keras.layers.Flatten()(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)


model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_rnn, Y_val))
# model.save('lstm_model10.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))

#######################################################################################
# CNN-LSTM Model 1 Architecture
inputs = keras.Input(shape=(1000, 102, 1))
x = keras.layers.Conv2D(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(inputs)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.AveragePooling2D(pool_size=2)(x)
x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu')(x)
x = keras.layers.Dropout(0.3)(x)
x = keras.layers.Conv2D(filters=1, kernel_size=3, strides=1, padding='same', activation='relu')(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Reshape((125, 13))(x)
x = keras.layers.LSTM(32)(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()
opt = keras.optimizers.Adam(lr=0.001)
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
training= model.fit(X_train_cnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_cnn, Y_val))
# model.save('cnn_model5.h5')
y_test = np.add(y_test, model.predict(X_test_cnn))

#######################################################################################
# CNN-LSTM Model 2 Architecture

inputs = keras.Input(shape=(1000, 102, 1))

x = keras.layers.AveragePooling2D(pool_size = (10, 1), batch_input_shape=(None, 1000, 102, 1))(inputs)
x = keras.layers.Reshape((100, 102))(x)
x = keras.layers.CuDNNLSTM(units = 32, return_sequences = True)(x)
x = keras.layers.Reshape((100, 32, 1))(x)
x = keras.layers.AveragePooling2D(pool_size = (3, 1))(x)

x = keras.layers.Conv2D(filters=32, kernel_size = (3, 3), strides=(1, 1), padding='same', activation='tanh')(x)
x = keras.layers.AveragePooling2D(pool_size = (2, 2))(x)
x = keras.layers.BatchNormalization()(x)

x = keras.layers.Conv2D(filters=32, kernel_size = (3, 3), strides=(1, 1), padding='same', activation='tanh')(x)
x = keras.layers.AveragePooling2D(pool_size = (2, 2))(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)


model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training = model.fit(X_train_cnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_cnn, Y_val))
# model.save('cnn_lstm_model6.h5')
y_test = np.add(y_test, model.predict(X_test_cnn))

#######################################################################################
# CNN-LSTM Model 3 Architecture
inputs = keras.Input(shape=(1000, 102, 1))

x = keras.layers.AveragePooling2D(pool_size = (4, 1), batch_input_shape=(None, 1000, 102, 1))(inputs)
x = keras.layers.Reshape((250, 102))(x)
x = keras.layers.CuDNNLSTM(units = 64, return_sequences = True)(x)
x = keras.layers.Reshape((250, 64, 1))(x)
x = keras.layers.AveragePooling2D(pool_size = (3, 1))(x)

x = keras.layers.Conv2D(filters=32, kernel_size = (3, 3), strides=(1, 1), padding='same', activation='tanh')(x)
x = keras.layers.AveragePooling2D(pool_size = (2, 2))(x)
x = keras.layers.BatchNormalization()(x)

x = keras.layers.Conv2D(filters=32, kernel_size = (3, 3), strides=(1, 1), padding='same', activation='tanh')(x)
x = keras.layers.AveragePooling2D(pool_size = (2, 2))(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)


model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training = model.fit(X_train_cnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_cnn, Y_val))
# model.save('cnn_lstm_model7.h5')
y_test = np.add(y_test, model.predict(X_test_cnn))

#######################################################################################
# Bidirectional-LSTM Model 1 Architecture
inputs = keras.Input(shape=(1000, 102))
x = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(inputs)
x = Dropout(0.3)(x)
x = Bidirectional(CuDNNLSTM(units=64))(x)
x = Dropout(0.3)(x)
x = Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])
training= model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_rnn, Y_val))
# model.save('lstm_model2.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))

#######################################################################################
# Bidirectional-LSTM Model 2 Architecture
inputs = keras.Input(shape=(1000, 102))
x = Bidirectional(CuDNNLSTM(units=128, return_sequences=True))(inputs)
x = Dropout(0.1)(x)
x = Bidirectional(CuDNNLSTM(units=64))(x)
x = Dropout(0.1)(x)
x = Dense(128, activation='softmax')(x)
x = Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)

model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_rnn, Y_val))
# model.save('lstm_model3.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))

#######################################################################################
# Bidirectional-LSTM Model 3 Architecture

inputs = keras.Input(shape=(1000, 102))
x = keras.layers.Bidirectional(CuDNNLSTM(units = 512, return_sequences=True), batch_input_shape=(None, 1000, 102))(inputs)
x = keras.layers.GlobalAveragePooling1D()(x)
x = keras.layers.Dropout(rate=0.2)(x)
x = keras.layers.Dense(128, activation='relu')(x)
x = keras.layers.Dropout(rate=0.2)(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)


model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training = model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_rnn, Y_val))
# model.save('pool_lstm_model8.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))

#######################################################################################
# Bidirectional-LSTM Model 4 Architecture
inputs = keras.Input(shape=(1000, 102))
x = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(units=64,return_sequences=False))(inputs)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=16,  validation_data=(X_val_rnn, Y_val))
# model.save('bidirect_model10.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))

#######################################################################################
# ResNet Model Architecture
def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):
    shortcut = y

    # down-sampling is performed with a stride of 2
    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same', kernel_regularizer = l2(1e-3))(y)
    y = BatchNormalization()(y)
    y = LeakyReLU()(y)

    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer = l2(1e-3))(y)
    y = BatchNormalization()(y)

    # identity shortcuts used directly when the input and output are of the same dimensions
    if _project_shortcut or _strides != (1, 1):
        # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)
        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2
        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same', kernel_regularizer = l2(1e-3))(shortcut)
        shortcut = BatchNormalization()(shortcut)

    y = add([shortcut, y])
    y = LeakyReLU()(y)

    return y

inputs = keras.Input(shape=(1000, 102, 1))
x = AveragePooling2D(pool_size = (10, 1), batch_input_shape=(None, 1000, 102, 1))(inputs)

x = residual_block(x, 32)
x = AveragePooling2D(pool_size=(2, 2))(x)

x = residual_block(x, 32)
x = AveragePooling2D(pool_size=(2, 2))(x)

x = Flatten()(x)
x = Dense(1 ,activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
# model.load_weights('resnet_model9.h5')
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_cnn, Y_train, epochs=nb_epoch, batch_size=128,  validation_data=(X_val_cnn, Y_val))
# model.save('resnet_model9.h5')
y_test = np.add(y_test, model.predict(X_test_cnn))

#######################################################################################
# Attention Model 1 Architecture
inputs = keras.Input(shape=(1000, 102))
x = keras.layers.CuDNNLSTM(units=32, return_sequences=True)(inputs)
x = SeqSelfAttention(attention_activation='sigmoid', return_attention=True)(x)
x = keras.layers.concatenate(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.summary()

#model.load_weights('attention_model7.h5')
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=32,  validation_data=(X_val_rnn, Y_val))
# model.save('attention_model7.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))

#######################################################################################
# Attention Model 2 Architecture
inputs = keras.Input(shape=(1000, 102, 1))
x = keras.layers.AveragePooling2D(pool_size=(10, 1))(inputs)
x = keras.layers.Reshape((100, 102))(x)
x = keras.layers.Bidirectional(keras.layers.LSTM(units=16,
                                                    return_sequences=True))(x)
x = SeqSelfAttention(attention_activation='sigmoid', return_attention=False)(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
#model.load_weights('attention_model8.h5')
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_attention, Y_train, epochs=nb_epoch, batch_size=32,  validation_data=(X_val_attention, Y_val))
# model.save('attention_model8.h5')
y_test = np.add(y_test, model.predict(X_test_attention))

#######################################################################################
# Attention Model 3 Architecture
inputs = keras.Input(shape=(1000, 102))
x = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(units=16,return_sequences=True))(inputs)
x = SeqSelfAttention(attention_activation='sigmoid', return_attention=False)(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(256, activation='relu')(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
#model.load_weights('attention_model9.h5')
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_rnn, Y_train, epochs=nb_epoch, batch_size=16,  validation_data=(X_val_rnn, Y_val))
# model.save('attention_model9.h5')
y_test = np.add(y_test, model.predict(X_test_rnn))

#######################################################################################
# Attention Model 4 Architecture
inputs = keras.Input(shape=(1000, 102, 1))
x = keras.layers.AveragePooling2D(pool_size=(10, 1))(inputs)
x = keras.layers.Reshape((100, 102))(x)
x = keras.layers.Bidirectional(keras.layers.LSTM(units=16,
                                                    return_sequences=True))(x)
x = SeqSelfAttention(
        attention_width=25,
        attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,
        attention_activation=None,
        kernel_regularizer=keras.regularizers.l2(1e-6),
        use_attention_bias=False,
        name='Attention',
    )(x)

x = keras.layers.Flatten()(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dense(64, activation='relu')(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
#model.load_weights('attention_model11.h5')
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_attention, Y_train, epochs=nb_epoch, batch_size=32,  validation_data=(X_val_attention, Y_val))
# model.save('attention_model11.h5')
y_test = np.add(y_test, model.predict(X_test_attention))

#######################################################################################
# Attention Model 5 Architecture
inputs = keras.Input(shape=(1000, 102, 1))
x = keras.layers.MaxPooling2D(pool_size=(4, 1))(inputs)
x = keras.layers.Reshape((250, 102))(x)
x = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(units=8,
                                                    return_sequences=True))(x)
x = SeqSelfAttention(
        attention_width=8,
        attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,
        attention_activation=None,
        kernel_regularizer=keras.regularizers.l2(1e-6),
        use_attention_bias=False,
        name='Attention',
    )(x)

x = keras.layers.Flatten()(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dense(32, activation='relu')(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
# model.load_weights('attention_model12.h5')
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_attention, Y_train, epochs=nb_epoch, batch_size=32,  validation_data=(X_val_attention, Y_val))
#model.save('attention_model12.h5')
y_test = np.add(y_test, model.predict(X_test_attention))

#######################################################################################
# Attention Model 6 Architecture
inputs = keras.Input(shape=(1000, 102, 1))
x = keras.layers.MaxPooling2D(pool_size=(2, 1))(inputs)
x = keras.layers.Reshape((500, 102))(x)
x = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(units=32,
                                                    return_sequences=True))(x)
x = SeqSelfAttention(
        attention_width=32,
        attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,
        attention_activation=None,
        kernel_regularizer=keras.regularizers.l2(1e-6),
        use_attention_bias=False,
        name='Attention',
    )(x)

x = keras.layers.Flatten()(x)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
#model.load_weights('attention_model13.h5')
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_attention, Y_train, epochs=nb_epoch, batch_size=32,  validation_data=(X_val_attention, Y_val))
#model.save('attention_model13.h5')
y_test = np.add(y_test, model.predict(X_test_attention))

#######################################################################################
# Attention Model 7 Architecture
inputs = keras.Input(shape=(1000, 102, 1))
x = keras.layers.AveragePooling2D(pool_size=(20, 1))(inputs)
x = keras.layers.Reshape((50, 102))(x)
x = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(units=32,
                                                    return_sequences=True))(x)
x = SeqSelfAttention(
        attention_width=32,
        attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,
        attention_activation=None,
        kernel_regularizer=keras.regularizers.l2(1e-6),
        use_attention_bias=False,
        name='Attention',
    )(x)

x = keras.layers.Flatten()(x)
x = keras.layers.Dense(1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=x)
model.load_weights('attention_model14.h5')
model.summary()
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

training= model.fit(X_train_attention, Y_train, epochs=nb_epoch, batch_size=32,  validation_data=(X_val_attention, Y_val))
# model.save('attention_model14.h5')
y_test = np.add(y_test, model.predict(X_test_attention))


#######################################################################################
# Process test predictions
num_models = 20

y_test = np.divide(y_test, num_models)
y_test=y_test.reshape(y_test.shape[0],)

# Infer the test prediction
d = []
for i in range(max_id):
    y = {'Id': i,
         'Predicted': y_test[i]
        }               
    d.append({'Id': i, 'Predicted': y_test[i]})
d=pd.DataFrame(d)
print("printing prediction...")
export_csv = d.to_csv ('20_model_ensemble.csv', index = None, header=True)  
print("output file generated.")
